{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ewx_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import socket\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import urllib\n",
    "from retrying import retry\n",
    "import pprint\n",
    "\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "#from auth import DEFAULT_API_ROOT\n",
    "from auth.external_service import ExternalService\n",
    "import parser\n",
    "#from tags import parse_tags\n",
    "#from timeseries import parse_result_df, parse_nested_structure_result_df\n",
    "\n",
    "logger = logging.getLogger()\n",
    "pp = pprint.PrettyPrinter(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
      " 'APPDATA': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Roaming',\n",
      " 'CLICOLOR': '1',\n",
      " 'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
      " 'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
      " 'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
      " 'COMPUTERNAME': 'HOU8804-DT03',\n",
      " 'COMSPEC': 'C:\\\\windows\\\\system32\\\\cmd.exe',\n",
      " 'CONDA_DEFAULT_ENV': 'la-tools',\n",
      " 'CONDA_EXE': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Scripts\\\\conda.exe',\n",
      " 'CONDA_PREFIX': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools',\n",
      " 'CONDA_PREFIX_1': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3',\n",
      " 'CONDA_PROMPT_MODIFIER': '(la-tools) ',\n",
      " 'CONDA_PYTHON_EXE': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\python.exe',\n",
      " 'CONDA_SHLVL': '2',\n",
      " 'FP_NO_HOST_CHECK': 'NO',\n",
      " 'GIT_PAGER': 'cat',\n",
      " 'HOMEDRIVE': 'C:',\n",
      " 'HOMEPATH': '\\\\Users\\\\wb5888',\n",
      " 'IPY_INTERRUPT_EVENT': '1740',\n",
      " 'IWBPATH': 'C:\\\\Program Files (x86)\\\\SAP\\\\FrontEnd\\\\iwb',\n",
      " 'JPY_INTERRUPT_EVENT': '1740',\n",
      " 'JPY_PARENT_PID': '1732',\n",
      " 'KERNEL_LAUNCH_TIMEOUT': '40',\n",
      " 'LDMS_LOCAL_DIR': 'C:\\\\Program Files (x86)\\\\LANDesk\\\\LDClient\\\\Data',\n",
      " 'LDMS_PREFERRED_SERVER': 'Dmayeux-EPM-TST.d90.tes.local',\n",
      " 'LOCALAPPDATA': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local',\n",
      " 'LOGONSERVER': '\\\\\\\\NADC08VP',\n",
      " 'MPLBACKEND': 'module://ipykernel.pylab.backend_inline',\n",
      " 'NUMBER_OF_PROCESSORS': '4',\n",
      " 'OS': 'Windows_NT',\n",
      " 'PAGER': 'cat',\n",
      " 'PATH': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools\\\\Library\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools\\\\Scripts;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\envs\\\\la-tools\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\condabin;C:\\\\Program '\n",
      "         'Files (x86)\\\\Common '\n",
      "         'Files\\\\Oracle\\\\Java\\\\javapath;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1\\\\bin;C:\\\\oracle\\\\product\\\\11.2.0\\\\client_1\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\windows\\\\system32;C:\\\\windows;C:\\\\windows\\\\System32\\\\Wbem;C:\\\\LODESTAR\\\\Bin;C:\\\\LODESTAR\\\\LTMH\\\\Runtime\\\\jre\\\\bin\\\\server;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program '\n",
      "         'Files (x86)\\\\Enterprise '\n",
      "         'Vault\\\\EVClient;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program '\n",
      "         'Files\\\\dotnet;C:\\\\Program Files\\\\Microsoft SQL '\n",
      "         'Server\\\\130\\\\Tools\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL '\n",
      "         'Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn;C:\\\\Program '\n",
      "         'Files\\\\PuTTY;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Continuum\\\\anaconda3\\\\Scripts;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1\\\\bin;C:\\\\oracle\\\\product\\\\11.2.0\\\\client_1\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\windows\\\\system32;C:\\\\windows;C:\\\\windows\\\\System32\\\\Wbem;C:\\\\LODESTAR\\\\Bin;C:\\\\LODESTAR\\\\LTMH\\\\Runtime\\\\jre\\\\bin\\\\server;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program '\n",
      "         'Files (x86)\\\\Enterprise '\n",
      "         'Vault\\\\EVClient;C:\\\\windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Program '\n",
      "         'Files\\\\dotnet;C:\\\\Program Files\\\\Microsoft SQL '\n",
      "         'Server\\\\130\\\\Tools\\\\Binn;C:\\\\Program Files\\\\Microsoft SQL '\n",
      "         'Server\\\\Client SDK\\\\ODBC\\\\170\\\\Tools\\\\Binn;C:\\\\Program '\n",
      "         'Files\\\\PuTTY;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36\\\\Scripts;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python36;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1;C:\\\\oracle\\\\64Bit\\\\product\\\\12.2.0\\\\client_1\\\\bin;C:\\\\oracle\\\\product\\\\11.2.0\\\\client_1\\\\bin;C:\\\\ProgramData\\\\Oracle\\\\Java\\\\javapath;C:\\\\windows\\\\system32;C:\\\\windows;C:\\\\windows\\\\System32\\\\Wbem;C:\\\\LODESTAR\\\\Bin;C:\\\\LODESTAR\\\\LTMH\\\\Runtime\\\\jre\\\\bin\\\\server;C:\\\\window;C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Programs\\\\Microsoft '\n",
      "         'VS Code\\\\bin',\n",
      " 'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
      " 'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
      " 'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 60 Stepping 3, GenuineIntel',\n",
      " 'PROCESSOR_LEVEL': '6',\n",
      " 'PROCESSOR_REVISION': '3c03',\n",
      " 'PROGRAMDATA': 'C:\\\\ProgramData',\n",
      " 'PROGRAMFILES': 'C:\\\\Program Files',\n",
      " 'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
      " 'PROGRAMW6432': 'C:\\\\Program Files',\n",
      " 'PROMPT': '(la-tools) $P$G',\n",
      " 'PSMODULEPATH': 'C:\\\\windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules\\\\;C:\\\\Program '\n",
      "                 'Files\\\\Microsoft Application Virtualization\\\\Client\\\\',\n",
      " 'PUBLIC': 'C:\\\\Users\\\\Public',\n",
      " 'PYTHON': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python37-32\\\\',\n",
      " 'SESSIONNAME': 'Console',\n",
      " 'SNC_LIB': 'C:\\\\Program Files '\n",
      "            '(x86)\\\\SAP\\\\SNCEncryption\\\\x86\\\\sapsncencryption.dll',\n",
      " 'SNC_LIB_64': 'C:\\\\Program Files '\n",
      "               '(x86)\\\\SAP\\\\SNCEncryption\\\\x64\\\\sapsncencryption.dll',\n",
      " 'SYSTEMDRIVE': 'C:',\n",
      " 'SYSTEMROOT': 'C:\\\\windows',\n",
      " 'TEMP': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Temp',\n",
      " 'TERM': 'xterm-color',\n",
      " 'TMP': 'C:\\\\Users\\\\wb5888\\\\AppData\\\\Local\\\\Temp',\n",
      " 'TNS_ADMIN': '\\\\\\\\mytna.com\\\\dfsroot\\\\tnsname',\n",
      " 'USERDNSDOMAIN': 'MYTNA.COM',\n",
      " 'USERDOMAIN': 'MYTNA',\n",
      " 'USERNAME': 'wb5888',\n",
      " 'USERPROFILE': 'C:\\\\Users\\\\wb5888',\n",
      " 'WINDIR': 'C:\\\\windows',\n",
      " 'WINDOWS_TRACING_FLAGS': '3',\n",
      " 'WINDOWS_TRACING_LOGFILE': 'C:\\\\BVTBin\\\\Tests\\\\installpackage\\\\csilogfile.log'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dict(os.environ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EWX(object):\n",
    "\n",
    "    def __init__(self, namespace_id=None, api_root=None, credentials=None):\n",
    "        super(EWX, self).__init__()\n",
    "        if not namespace_id:\n",
    "            namespace_id = os.environ.get('EWX_NAMESPACE')\n",
    "            logger.warning(\"No namespace parameter could be found, so we are using the primary namespace of the user\")\n",
    "        if not api_root:\n",
    "            api_root = DEFAULT_API_ROOT\n",
    "        self.namespace_id = namespace_id\n",
    "        self.api_root = api_root\n",
    "        logger.info('Using namespace = %s and api_root = %s', namespace_id, api_root)\n",
    "        self.client = ExternalService.get_client(api_root=api_root, credentials=credentials)\n",
    "\n",
    "    # ChannelClassifier\n",
    "    def get_channel_classifier(self, name):\n",
    "        \"\"\"Get a channel classifier by name\n",
    "\n",
    "        Args:\n",
    "            name (str): The name of the channel classifier\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary describing the channel classifier\n",
    "\n",
    "        \"\"\"\n",
    "        request = self.client.channelclassifier().classifier().get(name=name)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def create_channel_classifier(self, id, name, unit_type, datapoint_type, description=None):\n",
    "        \"\"\"Create a channel classifier by name\n",
    "\n",
    "        Args:\n",
    "            id (str):\n",
    "            name (str): The name of the channel classifier\n",
    "            unit_type (str):\n",
    "            datapoint_type (str):\n",
    "            description (str):\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary describing the channel calssifier\n",
    "\n",
    "        \"\"\"\n",
    "        body = {'id': name,\n",
    "                'unitType': unit_type,\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'datapointType': datapoint_type}\n",
    "        request = self.client.channelclassifier().classifier().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # RunConfig\n",
    "    def create_flowconfig(self, name, description=None, enabled=None, flow_type=\"scenario\", sequence_configs=None, destination_channel_classifier=None):\n",
    "        \"\"\" Creates a new flow configuration.\n",
    "\n",
    "        A flow configuration can consist of several sequence configurations that are\n",
    "        executed one after the other. An example sequence_configs argument is:\n",
    "        sequence_configs = [\n",
    "          {\n",
    "            \"name\": \"sequence1\",\n",
    "            \"description\": \"sequence 1\",\n",
    "            \"sourceColumn\": \"RAW\",\n",
    "            \"destinationColumn\": \"VEE_RESULT\",\n",
    "            \"ruleConfigs\": [\n",
    "              {\n",
    "                \"function\": \"gap_check\",\n",
    "                \"type\": \"validation\",\n",
    "                \"displayName\": \"Gap Check\"\n",
    "              },\n",
    "              {\n",
    "                \"function\": \"zero_reads\",\n",
    "                \"type\": \"validation\",\n",
    "                \"displayName\": \"Zero Reads\"\n",
    "              },\n",
    "            ]\n",
    "          },\n",
    "          {\n",
    "            \"name\": \"sequence2\",\n",
    "            \"description\": \"sequence 2\",\n",
    "            \"sourceColumn\": \"VEE_RESULT\",\n",
    "            \"destinationColumn\": \"VEE_RESULT\",\n",
    "            \"ruleConfigs\": [\n",
    "              {\n",
    "                \"function\": \"like_day_substitution\",\n",
    "                \"type\": \"validation\",\n",
    "                \"displayName\": \"Like Day Substitution\",\n",
    "                \"filterWhitelist\": [gap_check:0, zero_reads:0]\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the flow configuration.\n",
    "            description (str): Description of the flow configuration.\n",
    "            enabled (bool): A boolean indicating whether the configuration can be used.\n",
    "            flow_type (str): \"scenario\" or \"continuous\"\n",
    "            sequence_configs (list[dict]): A list of sequence configuration dictionaries.\n",
    "                Each sequence configuration can have a name, description,\n",
    "                destination column and a ruleConfigs key that takes a list.\n",
    "            destination_channel_classifier (str): classifier that will be used to store\n",
    "                flow metadata with. It's optional; if it is not provided, the destinationColumn\n",
    "                of the latest sequence will be used as the destination_channel_classifier.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the created flow configuration and its ID.\n",
    "        \"\"\"\n",
    "        body = {'name': name,\n",
    "                'description': description,\n",
    "                'enabled': enabled,\n",
    "                'flowType': flow_type,\n",
    "                'sequenceConfigs': sequence_configs,\n",
    "                'destinationChannelClassifier': destination_channel_classifier}\n",
    "        request = self.client.runconfig().runconfig().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_upload_url(self, market_adapter_id=None, tags=None, use_streaming=False):\n",
    "        request = self.client.storage().storage().files().uploadurl(marketAdapterId=market_adapter_id, tags=tags, useStreaming=use_streaming)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_flowconfig(self, id):\n",
    "        \"\"\" Gets a flow configuration by identifier.\n",
    "\n",
    "        Args:\n",
    "            id (str): The ID of the flow configuration.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary describing the flow configuration.\n",
    "        \"\"\"\n",
    "        request = self.client.runconfig().runconfig().get(id=id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def update_flowconfig(self, id, name, description=None, enabled=None, flow_type='scenario', sequence_configs=None, destination_channel_classifier=None):\n",
    "        \"\"\"Updates a flow configuration with the given ID using the parameters given.\n",
    "\n",
    "        One or more arguments can be updated at once, unused arguments are not affected.\n",
    "\n",
    "        Args:\n",
    "            id (str) The ID of the flow configuration.\n",
    "            name (str): Name of the flow configuration.\n",
    "            description (str): Description of the flow configuration.\n",
    "            enabled (bool): A boolean indicating whether the configuration can be used.\n",
    "            flow_type (str): \"scenario\" or \"continuous\"\n",
    "            sequence_configs (list[dict]): A list of sequence configuration dictionaries.\n",
    "                Each sequence configuration can have a name, description,\n",
    "                destination column and a ruleConfigs key that takes a list.\n",
    "            destination_channel_classifier (str): classifier that will be used to store\n",
    "                flow metadata with. It's optional; if it is not provided, the destinationColumn\n",
    "                of the latest sequence will be used as the destination_channel_classifier.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary describing the updated flow configuration.\n",
    "        \"\"\"\n",
    "        body = {'id': id,\n",
    "                'name': name,\n",
    "                'description': description,\n",
    "                'enabled': enabled,\n",
    "                'flowType': flow_type,\n",
    "                'sequenceConfigs': sequence_configs,\n",
    "                'destinationChannelClassifier': destination_channel_classifier}\n",
    "        request = self.client.runconfig().runconfig().update(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_flowconfigs(self, enabled=True, deleted=False, limit=20, page_token=None):\n",
    "        \"\"\" Gets a list of flow configurations that match the given filters.\n",
    "\n",
    "        Args:\n",
    "            enabled (bool): A boolean describing whether the flow configuration can be used.\n",
    "            deleted (bool): A boolean describing whether the flow configuration is deleted.\n",
    "            limit (int): How many results to return as an int.\n",
    "            page_token (None or str): A page_token identifier to retrieve more results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with flow configurations, possibly a pageToken and an\n",
    "                identifier whether there are more results.\n",
    "        \"\"\"\n",
    "        request = self.client.runconfig().runconfig().list(enabled=enabled, deleted=deleted, limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def delete_flowconfig(self, id):\n",
    "        \"\"\" Deletes a flow configuration with the given ID.\n",
    "\n",
    "        Args:\n",
    "            id (str): The ID of the flow configuration.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the flow configuration including a key deleted.\n",
    "        \"\"\"\n",
    "        request = self.client.runconfig().runconfig().delete(id=id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # Rules\n",
    "    def create_rules(self, items):\n",
    "        \"\"\" Creates rules based on the specification(s) in items.\n",
    "\n",
    "        Args:\n",
    "            items (list[dict]): A list of rule specifications.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with number of successfully created rules.\n",
    "        \"\"\"\n",
    "        body = {'items': items}\n",
    "        request = self.client.rule().rule().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_rule(self, name):\n",
    "        \"\"\" Gets a rule description based on its name.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the rule parameters.\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().get(name=name)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_rules(self, rule_type):\n",
    "        \"\"\" Lists all rules of specified type.\n",
    "\n",
    "        Args:\n",
    "            rule_type (str): Which type of rules to list.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary which contains a list of rule dictionaries, possibly a page token\n",
    "            and an indicator whether there are more results.\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().list(ruleType=rule_type)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def update_rule(self, name, rule_type, display_name=None, description=None, params=None, code_blob=None):\n",
    "        \"\"\" Updates an existing rule defined by its name and type.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the rule.\n",
    "            rule_type (str): Type of the rule.\n",
    "            display_name (str): Display name of the rule.\n",
    "            description (str): Description of the rule.\n",
    "            params (list): Rule parameters.\n",
    "            code_blob (str): Code blob describing the rule (not yet available).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the updated rule parameters.\n",
    "        \"\"\"\n",
    "        body = {'name': name,\n",
    "                'displayName': display_name,\n",
    "                'description': description,\n",
    "                'ruleType': rule_type,\n",
    "                'params': params,\n",
    "                'codeBlob': code_blob}\n",
    "        request = self.client.rule().rule().update(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def remove_rule(self, name):\n",
    "        \"\"\" Removes a rule.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict: An empty dictionary.\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().delete(name=name)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_ruletypes(self, limit=20, page_token=None):\n",
    "        \"\"\" Lists rule types.\n",
    "\n",
    "        Args:\n",
    "            limit (int): How many rule types to return.\n",
    "            page_token (None or str): A token to fetch further results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of rule types.\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().ruletype().list(limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def prototype_rule(self, name):\n",
    "        \"\"\" Prepares a rule for prototyping.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the name of the rule\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().prototype(name=name, body={})\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def release_rule(self, name):\n",
    "        \"\"\" Releases a prototyped rule.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the rule.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the name of the rule\n",
    "        \"\"\"\n",
    "        request = self.client.rule().rule().release(name=name, body={})\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # Datasource\n",
    "    def create_datasources(self, body):\n",
    "        \"\"\" Creates new datasources with the given properties.\n",
    "\n",
    "        Args:\n",
    "            body(dict): the datasources request body\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the ID and creation time of the new datasources.\n",
    "        \"\"\"\n",
    "        request = self.client.flow().datasources().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def create_datasource(self, body):\n",
    "        \"\"\" Creates a new datasource with the given properties.\n",
    "\n",
    "        Args:\n",
    "            body(dict): the datasource request body\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the ID and creation time of the new datasource.\n",
    "        \"\"\"\n",
    "        request = self.client.datasource().datasource().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_datasource(self, id):\n",
    "        \"\"\" Gets a datasource by identifier.\n",
    "\n",
    "        Args:\n",
    "            id (str): A datasource identifier as a string.\n",
    "\n",
    "        Returns:\n",
    "            dict: A datasource dictionary.\n",
    "        \"\"\"\n",
    "        request = self.client.datasource().datasource().get(id=id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_datasources(self, limit=20, page_token=None):\n",
    "        \"\"\" Lists datasources.\n",
    "\n",
    "        Args:\n",
    "            limit (int): How many datasources to return at most as an integer.\n",
    "            page_token (str or None):\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with a list of datasource dictionaries, possibly\n",
    "                a page token and an indicator whether there are more results.\n",
    "        \"\"\"\n",
    "        request = self.client.datasource().datasources().list(limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_channel_classifiers(self, limit=None, order=None, page_token=None):\n",
    "        \"\"\" Lists channel classifiers.\n",
    "\n",
    "        Args:\n",
    "            limit (int): How many channel classifiers to return.\n",
    "            order (str): A key of channel classifier dictionary to use for sorting.\n",
    "            page_token (str or None): A token to fetch further results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with a list of channel classifier dictionaries and\n",
    "                possibly a page token.\n",
    "        \"\"\"\n",
    "        request = self.client.datasource().channels().classifier().list(limit=limit, order=order, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def update_channel_classifier(self, name, new_name, description):\n",
    "        \"\"\" Updates a channel classifier.\n",
    "\n",
    "        Args:\n",
    "            name (str): Name of the channel classifier.\n",
    "            new_name (str): New name of the channel classifier.\n",
    "            description (str): New description of the channel classifier.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the updated channel classifier.\n",
    "        \"\"\"\n",
    "        body = {'name': name,\n",
    "                'newName': new_name,\n",
    "                'description': description}\n",
    "        request = self.client.datasource().channels().classifier().update(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def add_channels(self, id, channels):\n",
    "        \"\"\" Adds channels to a datasource.\n",
    "\n",
    "        Args:\n",
    "            id (str): The ID of the datasource.\n",
    "            channels (list[dict]): A list of channels.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the ID of the datasource and update time.\n",
    "        \"\"\"\n",
    "        body = {'id': id,\n",
    "                'channels': channels}\n",
    "        request = self.client.datasource().channels().add(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def remove_channels(self, id, channels):\n",
    "        \"\"\" Removes channels from a datasource.\n",
    "\n",
    "        Args:\n",
    "            id (str): The ID of the datasource.\n",
    "            channels (list): A list of channels to be removed. Each dictionary\n",
    "                needs to contain at least the id and classifier of the channel.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the ID of the datasource and update time.\n",
    "        \"\"\"\n",
    "        body = {'id': id,\n",
    "                'channels': channels}\n",
    "        request = self.client.datasource().channels().remove(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # Query\n",
    "    def execute_query(self, query, job_id=None, limit=10, page_token=None, raw_result=False, priority='interactive'):\n",
    "        \"\"\" Execute an EQL query.\n",
    "\n",
    "        Args:\n",
    "            query (str): The query as a string.\n",
    "            job_id (str): if provided, it will try to fetch the result from this job id\n",
    "            limit (int):\n",
    "            page_token (str or None):\n",
    "            raw_result (bool): return raw JSON result from API or parse into dataframe\n",
    "\n",
    "        Returns:\n",
    "            str or dict or pd.DataFrame: Results of the query in a dataframe.\n",
    "        \"\"\"\n",
    "        job_complete = False\n",
    "        while not job_complete:\n",
    "            request = self.client.query().query().execute(query=query, jobId=job_id, limit=limit, pageToken=page_token, priority=priority)\n",
    "            result = self.__execute_and_handle_response(request)\n",
    "            job_complete = result['reference'].get('jobComplete', True)\n",
    "            job_id = result['reference'].get('jobId')\n",
    "            time.sleep(0.1)\n",
    "        if isinstance(result, str) or raw_result:\n",
    "            return result\n",
    "        try:\n",
    "            return parse_result_df(result=result)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            return parse_nested_structure_result_df(result=result)\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            return parse_tags(result=result)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def search_files(self, filename=None, tags=None, read_only=None, created_date=None, user_id=None, market_adapter_id=None, limit=None, page_token=None):\n",
    "        request = self.client.storage().storage().files().search(filename=filename, tags=tags,\n",
    "                                                                 readOnly=read_only, createdDate=created_date,\n",
    "                                                                 userId=user_id, marketAdapterId=market_adapter_id,\n",
    "                                                                 limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def ingest_files(self, market_adapter_id=None, use_streaming=False, file_locations=None):\n",
    "        request = self.client.storage().storage().files().ingest(body=dict(marketAdapterId=market_adapter_id, useStreaming=use_streaming,\n",
    "                                                                 fileLocations=file_locations))\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "\n",
    "    # Tag\n",
    "    def list_tags(self, limit=None, page_token=None):\n",
    "        \"\"\" Lists used tags.\n",
    "\n",
    "        Args:\n",
    "            limit (int): How many results to return.\n",
    "            page_token (str or None): Retrieve more results starting from previous results.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with a list of used tags, possibly a page token and an\n",
    "                indicator whether there are more results.\n",
    "        \"\"\"\n",
    "        request = self.client.tag().tag().name().list(limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # TODO: Fix the API call (EWXP-1517)\n",
    "    # def get_tag_versions(self, datasource_ids, tag_filter, limit, page_token):\n",
    "    #     \"\"\"\n",
    "    #\n",
    "    #     Args:\n",
    "    #         datasource_ids:\n",
    "    #         tag_filter:\n",
    "    #         limit:\n",
    "    #         page_token:\n",
    "    #\n",
    "    #     Returns:\n",
    "    #\n",
    "    #     \"\"\"\n",
    "    #     request = self.client.tag().tag().versions().get(\n",
    "    #         datasourceIds=datasource_ids, tagFilter=tag_filter, limit=limit, pageToken=page_token)\n",
    "    #     return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def add_tags(self, tags):\n",
    "        \"\"\" Adds tags to a datasource.\n",
    "\n",
    "        Args:\n",
    "            tags: A list of tag dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the number of added tags and a creation time.\n",
    "        \"\"\"\n",
    "        request = self.client.tag().tag().add(body=tags)\n",
    "        return self.__execute_and_handle_response(request, async=True)\n",
    "\n",
    "    def update_tags(self, tags):\n",
    "        \"\"\" Updates tags of a datasource.\n",
    "\n",
    "        Args:\n",
    "            tags: A list of tag dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the number of updates tags and an update time.\n",
    "        \"\"\"\n",
    "        request = self.client.tag().tag().update(body=tags)\n",
    "        return self.__execute_and_handle_response(request, async=True)\n",
    "\n",
    "    def remove_tags(self, tags):\n",
    "        \"\"\" Removes tags of a datasource.\n",
    "\n",
    "        Args:\n",
    "            tags: A list of tag dictionaries.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with the number of removed tags and a removal time.\n",
    "        \"\"\"\n",
    "        request = self.client.tag().tag().remove(body=tags)\n",
    "        return self.__execute_and_handle_response(request, async=True)\n",
    "\n",
    "    def get_flow_metadata(self, flow_id):\n",
    "        \"\"\" Get flow metadata based on the flow id.\n",
    "\n",
    "        Args:\n",
    "            flow_id: The flow identifier as a string.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary of flow metadata.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('This functionality is not supported')\n",
    "\n",
    "    def list_flow_classifiers(self, datasource_id):\n",
    "        \"\"\" List flow classifiers based on a datasource id.\n",
    "\n",
    "        Args:\n",
    "            datasource_id: The datasource id as a string.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with flow classifiers in key items as a list and an indicator whether there are more results\n",
    "            in key more. If there are more results, also a pageToken is returned in the dictionary.\n",
    "        \"\"\"\n",
    "        # TODO: limit and pageToken arguments not working\n",
    "        request = self.client.run().run().classifier().datasource().list(datasourceId=datasource_id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_flows(self, datasource_ids=None, flow_id=None, flow_date=None, flow_type=None, channel_classifiers=None, completed=None, approved=None, page_token=None):\n",
    "        \"\"\" List flows for a datasource id.\n",
    "\n",
    "        Args:\n",
    "            datasource_ids list[str]: The datasource ids\n",
    "            flow_id (str): Search for an exact flow ID\n",
    "            flow_date (str): only search on a specific date\n",
    "            flow_type (str): \"scenario\" or \"continuous\"\n",
    "            channel_classifiers list[str]: Optional channel classifier id as a string to return only that channel classifier.\n",
    "            completed (bool): indicates to filter for only completed flows (if True)\n",
    "            approved (bool): indicates to filter for only approved flows (if True)\n",
    "            page_token (str or None): used when results are paged\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with flows in items key and indicator whether there are more results in key more.\n",
    "                If there are more results, also a pageToken is returned in the dictionary.\n",
    "        \"\"\"\n",
    "        job_id = None\n",
    "        job_complete = False\n",
    "        while not job_complete:\n",
    "            request = self.client.run().run().search(datasourceIds=datasource_ids, flowId=flow_id, flowDate=flow_date, flowType=flow_type, channelClassifiers=channel_classifiers, completed=completed, approved=approved, pageToken=page_token, jobId=job_id)\n",
    "            result = self.__execute_and_handle_response(request)\n",
    "            job_id = result['jobId']\n",
    "            job_complete = 'items' in result and result['jobId']\n",
    "            time.sleep(0.1)\n",
    "        return result\n",
    "\n",
    "    def start_flow(self, datasource_identifiers, source_classifier, flowconfiguration_id, start_datetime, end_datetime, use_streaming=False, persist=True):\n",
    "        \"\"\" Starts a new flow. Example request body that is being built up:\n",
    "\n",
    "        {\n",
    "            \"startDatetime\": \"string\",\n",
    "            \"endDatetime\": \"string\",\n",
    "            \"useStreaming\": true,\n",
    "            \"datasourceIds\": [\n",
    "                \"string\"\n",
    "            ],\n",
    "            \"persist\": true,\n",
    "            \"sourceClassifier\": \"string\",\n",
    "            \"flowConfigurationId\": \"string\"\n",
    "        }\n",
    "\n",
    "        Args:\n",
    "            datasource_identifiers (list[str]): datasource identifiers to start flows for\n",
    "            source_classifier (str): classifier to start the flow for\n",
    "            flowconfiguration_id (str): id of the flow configuration to use\n",
    "            start_datetime (str): start date of timeseries data to use. Format of datetime\n",
    "                %Y-%m-%dT%H:%M:%S.%f (example 2017-03-18T00:00:00.000). The time is assumed\n",
    "                to be in UTC.\n",
    "            end_datetime (str): end date of the timeseries data to use. Format of datetime\n",
    "                %Y-%m-%dT%H:%M:%S.%f (example 2018-02-24T00:00:00.000). The time is assumed\n",
    "                to be in UTC.\n",
    "            use_streaming (bool): whether to start the flow in streaming mode or in batch mode.\n",
    "                It is advised to start a lot of flows (a big batch) in batch mode. If you'd only\n",
    "                like to start one or a few flows, streaming is the way to go.\n",
    "            persist (bool): indicates whether to store the flow result. If not, then the result\n",
    "                can be polled later on. NOTE; THIS IS NOT SUPPORTED YET. BY DEFAULT IT WILL BE\n",
    "                SET TO TRUE FOR NOW.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary with referenceId and a list named datasourcesRunStarted.\n",
    "        \"\"\"\n",
    "        body = {\"startDatetime\": start_datetime,\n",
    "                \"endDatetime\": end_datetime,\n",
    "                \"useStreaming\": use_streaming,\n",
    "                \"datasourceIds\": datasource_identifiers,\n",
    "                \"persist\": True,\n",
    "                \"sourceClassifier\": source_classifier,\n",
    "                \"flowConfigurationId\": flowconfiguration_id}\n",
    "        request = self.client.run().run().start(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    # =================\n",
    "    # Transport Adapter\n",
    "    # =================\n",
    "\n",
    "    def create_transport_adapter(self, name, description, type, market_adapter_id, properties):\n",
    "        \"\"\" Create a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "            description (str):\n",
    "            type (str):\n",
    "            market_adapter_id (str):\n",
    "            properties (dict[str, str]):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        prepared_properties = [dict(key=kv, value=properties[kv]) for kv in properties] if properties else {}\n",
    "        body = dict(name=name, description=description, type=type, market_adapter_id=market_adapter_id, properties=prepared_properties)\n",
    "        request = self.client.transportadapters().transportadapters().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def list_transport_adapters(self, limit=None, page_token=None):\n",
    "        \"\"\" Retrieves a list of transport adapter configurations\n",
    "\n",
    "        Args:\n",
    "            limit (str):\n",
    "            page_token (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.transportadapters().transportadapters().list(limit=limit, pageToken=page_token)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_transport_adapter(self, transport_adapter_id):\n",
    "        \"\"\" Retrieves a single transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.transportadapters().transportadapters().get(id=transport_adapter_id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def update_transport_adapter(self, transport_adapter_id, name, description, type, market_adapter_id, properties):\n",
    "        \"\"\" Updates a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "            name (str):\n",
    "            description (str):\n",
    "            type (str):\n",
    "            market_adapter_id (str):\n",
    "            properties (list[dict[str, str]):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        prepared_properties = [dict(key=kv, value=properties[kv]) for kv in properties] if properties else {}\n",
    "        body = dict(name=name, description=description, type=type, market_adapter_id=market_adapter_id, properties=prepared_properties)\n",
    "        request = self.client.transportadapters().transportadapters().update(id=transport_adapter_id, body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def patch_transport_adapter(self, transport_adapter_id, description=None, type=None, market_adapter_id=None, properties=None, adapter_data=None):\n",
    "        \"\"\" Patches a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "            description (str):\n",
    "            type (str):\n",
    "            market_adapter_id (str):\n",
    "            properties (list[dict[str, str]):\n",
    "            adapter_data (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        body = dict()\n",
    "        if description is not None:\n",
    "            body['description'] = description\n",
    "        if type is not None:\n",
    "            body['type'] = type\n",
    "        if market_adapter_id is not None:\n",
    "            body['marketAdapterId'] = market_adapter_id\n",
    "        if properties is not None:\n",
    "            body['properties'] = [dict(key=kv, value=properties[kv]) for kv in properties] if properties else {}\n",
    "        if adapter_data is not None:\n",
    "            body['adapterData'] = adapter_data\n",
    "        request = self.client.transportadapters().transportadapters().patch(id=transport_adapter_id, body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def add_transport_adapter_property(self, transport_adapter_id, properties):\n",
    "        \"\"\" Adds properties to a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "            properties (dict[str, str]:\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        body = dict(properties=[dict(key=_key, value=properties[_key]) for _key in properties] if properties else {})\n",
    "        request = self.client.transportadapters().transportadapters().properties().add(id=transport_adapter_id, body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def add_channel_classifier(self, unit_type, description, name, datapoint_type):\n",
    "        \"\"\" Adds a channel classifier\n",
    "\n",
    "        Args:\n",
    "            unit_type (str):\n",
    "            description (bool):\n",
    "            name (str):\n",
    "            datapoint_type (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        body = dict(unitType=unit_type, description=description, name=name, datapointType=datapoint_type)\n",
    "        request = self.client.channelclassifier().classifier().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_transport_adapter_property(self, transport_adapter_id, key):\n",
    "        \"\"\" Retrieves a single property from a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "            key (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.transportadapters().transportadapters().properties().get(id=transport_adapter_id, key=key)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def delete_transport_adapter(self, transport_adapter_id):\n",
    "        \"\"\" Deletes a transport adapter configuration\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.transportadapters().transportadapters().delete(id=transport_adapter_id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def trigger_transport_adapter(self, transport_adapter_id):\n",
    "        \"\"\" Triggers a transport adapter\n",
    "\n",
    "        Args:\n",
    "            transport_adapter_id (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.transportadapters().transportadapters().trigger(id=transport_adapter_id, body={})\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def create_iot_register(self, name, description, number_of_streaming_buffers, streaming_buffer_window, market_adapter_id, buffer_processing_market_adapter_id):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            name (str):\n",
    "            description (str):\n",
    "            number_of_streaming_buffers (int):\n",
    "            streaming_buffer_window (int):\n",
    "            market_adapter_id (str):\n",
    "            buffer_processing_market_adapter_id (str):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        body = {\n",
    "            \"name\": name,\n",
    "            \"description\": description,\n",
    "            \"numberOfStreamingBuffers\": number_of_streaming_buffers,\n",
    "            \"streamingBufferWindow\": streaming_buffer_window,\n",
    "            \"marketAdapterId\": market_adapter_id,\n",
    "            \"bufferProcessingMarketAdapterId\": buffer_processing_market_adapter_id\n",
    "        }\n",
    "        request = self.client.iot().iot().registers().create(body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def create_iot_device(self, iot_register_name, device_id, blocked):\n",
    "        \"\"\" Will create an IOT device for the provided IOT register\n",
    "\n",
    "        Args:\n",
    "            iot_register_name (str):\n",
    "            device_id (str):\n",
    "            blocked (bool):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        body = {\n",
    "            \"id\": device_id,\n",
    "            \"blocked\": blocked\n",
    "        }\n",
    "        request = self.client.iot().iot().devices().create(iotRegisterName=iot_register_name, body=body)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def get_timeslice_group(self, timeslice_group_id):\n",
    "        \"\"\" Retrieves a timeslice group object\n",
    "\n",
    "        Args:\n",
    "            timeslice_group_id (str):\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "        \"\"\"\n",
    "        request = self.client.timeslice().timeslice().get(id=timeslice_group_id)\n",
    "        return self.__execute_and_handle_response(request)\n",
    "\n",
    "    def read_iot_buffer(self, register_name, device_id, number_of_buffers=None, column_filter=None, start_datetime=None):\n",
    "        \"\"\" Reads (an) iot buffer(s)\n",
    "\n",
    "        Args:\n",
    "            register_name (str):\n",
    "            device_id (str):\n",
    "            number_of_buffers (int):\n",
    "            column_filter (list[str]):\n",
    "            start_datetime (int):\n",
    "\n",
    "        Returns:\n",
    "            object:\n",
    "        \"\"\"\n",
    "        request = self.client.iot().iot().buffer().read(registerName=register_name,\n",
    "                                                        deviceId=device_id)\n",
    "        query_params = {}\n",
    "        if number_of_buffers is not None:\n",
    "            query_params['numberOfBuffers'] = number_of_buffers\n",
    "        if column_filter is not None:\n",
    "            query_params['columnFilter'] = column_filter\n",
    "        if start_datetime is not None:\n",
    "            query_params['startDatetime'] = start_datetime\n",
    "        if query_params:\n",
    "            request.uri += '&{}'.format(urllib.urlencode(query_params))\n",
    "        result = self.__execute_and_handle_response(request)\n",
    "        return parse_nested_structure_result_df(result=result)\n",
    "\n",
    "    def __update_namespace(self, request, async=False):\n",
    "        request.headers.update({'x-namespace': self.namespace_id})\n",
    "        if async:\n",
    "            request.headers.update({'x-async-request': async})\n",
    "        return request\n",
    "\n",
    "    def __execute_and_handle_response(self, request, async=False):\n",
    "        try:\n",
    "            updated_request = self.__update_namespace(request, async)\n",
    "        except Exception as ex:\n",
    "            logger.error('Namespace could not be updated: %s', ex, exc_info=True)\n",
    "            raise Exception('Namespace could not be updated: {}'.format(ex))\n",
    "        for count in range(2):\n",
    "            try:\n",
    "                return updated_request.execute(num_retries=3)\n",
    "            except socket.error as socket_error:\n",
    "                if socket_error.strerror == 'Connection reset by peer':\n",
    "                    logger.info('%s -> retry', socket_error.strerror)\n",
    "                    continue\n",
    "            except HttpError:\n",
    "                raise\n",
    "            except Exception as ex:\n",
    "                logger.error('Request failed: %s', ex, exc_info=True)\n",
    "                raise Exception('Request failed: {}'.format(ex))\n",
    "\n",
    "    def upload_file(self, filename, file_content=None, tags=None, adapter_id=None, streaming=False):\n",
    "        \"\"\"It is important to use the exact same way of uploading data when developing, so this function\n",
    "        uploads files using the EDC way. First we get the upload url to blobstore via the API (using the\n",
    "        EWX client) and then we upload to blobstore API. The prepare process is then triggered by GAE\n",
    "        when a market adapter id is given.\n",
    "\n",
    "        Args:\n",
    "            filename (str): The filename to use when uploading\n",
    "            file_content (str): The file contents to upload\n",
    "            tags (list[str]): Tags that needs to be assigned to the file to be uploaded so it can be found in the filemanager\n",
    "            adapter_id (str): The adapter id, if given, after the upload an ingest will be triggered with this market adapter id\n",
    "            streaming (bool): When set to true, it will use the streaming pipeline\n",
    "\n",
    "        Returns:\n",
    "            object\n",
    "        \"\"\"\n",
    "        def _retry_if_exception(exception):\n",
    "            \"\"\" Specify an exception you need. or just True\"\"\"\n",
    "            return isinstance(exception, RuntimeError)\n",
    "            \n",
    "        STOP_MAX_DELAY = 600000\n",
    "        WAIT_EXPONENTIAL_MAX = 10000\n",
    "        WAIT_EXPONENTIAL_MULTIPLIER = 1000\n",
    "        @retry(retry_on_exception=_retry_if_exception, wait_exponential_multiplier=WAIT_EXPONENTIAL_MULTIPLIER, wait_exponential_max=WAIT_EXPONENTIAL_MAX, stop_max_delay=STOP_MAX_DELAY)\n",
    "        def _do_upload(upload_url, files, data):\n",
    "            logging.info(\"Using upload_url: %s\", upload_url)\n",
    "            response = requests.post(upload_url, files={filename: file_content},\n",
    "                                     data=dict(adapter_id=adapter_id, streaming=streaming),\n",
    "                                     headers={'X-NAMESPACE': self.namespace_id, 'Accept': 'application/json,application/vnd.ewx.v2'})\n",
    "            if 200 <= response.status_code < 300:\n",
    "                logging.info('File %s successfully uploaded', filename)\n",
    "                return response\n",
    "            else:    \n",
    "                logging.error('File %s could not be uploaded. Error: %s %s', filename, response.status_code, response.reason)\n",
    "                raise RuntimeError('File {} could not be uploaded. Error: {} {}'.format(filename, response.status_code, response.reason))\n",
    "                \n",
    "        if not file_content:\n",
    "            raise RuntimeError(\"File content is required!\")\n",
    "        if tags is None:\n",
    "            tags = []\n",
    "        logger.info(\"Uploading %s to blobstore with size %s\", filename, len(file_content))\n",
    "        res = self.get_upload_url(tags=tags, market_adapter_id=adapter_id, use_streaming=streaming)\n",
    "        if not res:\n",
    "            raise RuntimeError(\"Did not get a valid response for upload url\")\n",
    "        upload_url = res.get('uploadUrl')\n",
    "        if not upload_url:\n",
    "            raise RuntimeError(\"Could not create an uploadUrl with filename %s, and adapter_id %s\", filename, adapter_id)\n",
    "        return _do_upload(upload_url, files={filename: file_content}, data=dict(adapter_id=adapter_id, streaming=streaming))\n",
    "\n",
    "    def download_file(self, blob_key):\n",
    "        request_url = self.api_root + '/files/get/' + blob_key\n",
    "        credentials = ExternalService._get_credentials(None)\n",
    "        import httplib2\n",
    "        http = httplib2.Http(disable_ssl_certificate_validation=True, timeout=60)\n",
    "        credentials.authorize(http)\n",
    "        response, content = http.request(request_url, method='GET', headers={\n",
    "            'X-NAMESPACE': self.namespace_id,\n",
    "            'Accept': 'application/json,application/vnd.ewx.v2',\n",
    "        })\n",
    "\n",
    "        if 200 <= response.status < 300:\n",
    "            return content\n",
    "        else:\n",
    "            raise RuntimeError('File download error: {} {}'.format(response.status, response.reason))\n",
    "\n",
    "\n",
    "class EWXPool(object):\n",
    "\n",
    "    _EWXPool__instance = None\n",
    "\n",
    "    # Singleton pattern\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        if not EWXPool._EWXPool__instance:\n",
    "            EWXPool._EWXPool__instance = object.__new__(cls)\n",
    "        return EWXPool._EWXPool__instance\n",
    "\n",
    "    def get_client(self, api_root=None, namespace_id=None, credentials=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            api_root (str or None):\n",
    "            namespace_id (str or None): the namespace\n",
    "            credentials (ServiceAccountCredentials or None): the credentials\n",
    "\n",
    "        Returns:\n",
    "            EWX: an EWX client\n",
    "        \"\"\"\n",
    "        if not namespace_id:\n",
    "            raise ValueError('namespace must be specified')\n",
    "\n",
    "        client_name = '__ewx_client_' + namespace_id\n",
    "        client = getattr(self, client_name, None)\n",
    "        # Check if there is already a client for this region/namespace and check if the credentials are still valid\n",
    "        if not client: # or client.credentials.expired:\n",
    "            client = EWX(api_root=api_root, namespace_id=namespace_id, credentials=credentials)\n",
    "            setattr(self, client_name, client)\n",
    "        return client\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
